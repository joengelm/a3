{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Classification from Raw Sensor Data\n",
    "\n",
    "This project describes how to take raw sensor data from PowerSense (and iOS app), break it into uniformly sized samples, featurize those samples in some way, and classify activities based on those chosen features.\n",
    "\n",
    "## Generate samples from raw data\n",
    "\n",
    "These two snippets use _pandas_ to read the CSV file containing raw sensor data and call `get_samples` to split the data frames into 10 second (i.e. 1000 data points at 100 Hz) samples. These samples are stored in a dictionary and are featurized in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_samples(data_points, points_per_sample):\n",
    "    return [data_points[i:i+points_per_sample] for i in range(0, len(data_points), points_per_sample)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 samples for sitting\n",
      "Found 30 samples for walking\n",
      "Found 30 samples for walking_holding_phone\n",
      "Found 30 samples for stairs\n",
      "Found 30 samples for car\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "POINTS_PER_SAMPLE = 1000\n",
    "\n",
    "activities = [\"sitting\", \"walking\", \"walking_holding_phone\", \"stairs\", \"car\"]\n",
    "\n",
    "activity_samples = {}    # Mapping from activity to list of 10 second data frames\n",
    "for activity in activities:\n",
    "    activity_samples[activity] = get_samples(pd.read_csv(activity + '.csv'), POINTS_PER_SAMPLE)\n",
    "    print \"Found\", len(activity_samples[activity]), \"samples for\", activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurize samples\n",
    "\n",
    "Below, I take each sample generated above and featurize it. The features I chose are the mean and variance of each of the fields provided by the sensors (12 fields, 24 features). I also tried using a Fourier Transform to calculate the frequency and wavelength for each sensor in the sample, but this negatively affected my classifiers, so I have commented it out.\n",
    "\n",
    "By the end of this snippet, the variables _X_ and _y_ contain the features and labels of the data. Also, _X\\_train_, _y\\_train_, _X\\_test_, and _y\\_test_ are provided for manual scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurization complete.\n"
     ]
    }
   ],
   "source": [
    "import numpy.fft as fft\n",
    "import numpy as np\n",
    "\n",
    "ACCELERATION_FIELDS = ['user_acc_x', 'user_acc_y', 'user_acc_z']\n",
    "ATTITUDE_FIELDS = ['attitude_roll', 'attitude_pitch', 'attitude_yaw']\n",
    "ROTATION_RATE_FIELDS = ['rotation_rate_x', 'rotation_rate_y', 'rotation_rate_z']\n",
    "GRAVITY_FIELDS = ['gravity_x', 'gravity_y', 'gravity_z']\n",
    "\n",
    "FIELDS = ACCELERATION_FIELDS + ATTITUDE_FIELDS + ROTATION_RATE_FIELDS + GRAVITY_FIELDS\n",
    "\n",
    "activity_features = {}    # Mapping from activity to list of samples' features\n",
    "for activity in activities:\n",
    "    activity_features[activity] = []\n",
    "    for sample in activity_samples[activity]:\n",
    "        # Find mean and variance\n",
    "        sample_features = [sample.mean()[field] for field in FIELDS]\n",
    "        sample_features += [sample.var()[field] for field in FIELDS]\n",
    "        \n",
    "        # Use FFT to find wavelength and frequency\n",
    "        #sample_features += [np.argmax(np.abs(fft.fft(sample[field]))) for field in FIELDS]\n",
    "        #sample_features += [max(np.abs(fft.fft(sample[field]))) for field in FIELDS]\n",
    "        \n",
    "        # Add all the features\n",
    "        activity_features[activity].append(sample_features)\n",
    "\n",
    "X = []\n",
    "X_train = []\n",
    "X_test = []\n",
    "for activity in activities:\n",
    "    X += activity_features[activity]\n",
    "    X_train += activity_features[activity][:26]\n",
    "    X_test += activity_features[activity][26:]\n",
    "\n",
    "# Generate labels\n",
    "y = [int(math.floor(float(i) / 150 * 5)) for i in range(0, 150)]\n",
    "y_train = [int(math.floor(float(i) / 130 * 5)) for i in range(0, 130)]\n",
    "y_test = [int(math.floor(float(i) / 20 * 5)) for i in range(0, 20)]\n",
    "\n",
    "print \"Featurization complete.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test classifiers\n",
    "\n",
    "Below, for each classifier I want to try, I import the classifier from SciKit Learn, initialize it (with default parameters, and cross validate it (with three folds). The accuracy is printed out with its 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.85 (+/- 0.10)\n",
      "Decision Tree: 0.92 (+/- 0.23)\n",
      "Logistic Regression: 0.95 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "# SVM\n",
    "from sklearn import svm\n",
    "svmClassifier = svm.SVC()\n",
    "svmScores = cross_validation.cross_val_score(svmClassifier, X, y)\n",
    "print(\"SVM: %0.2f (+/- %0.2f)\" % (svmScores.mean(), svmScores.std() * 2))\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtScores = cross_validation.cross_val_score(dtClassifier, X, y)\n",
    "print(\"Decision Tree: %0.2f (+/- %0.2f)\" % (dtScores.mean(), dtScores.std() * 2))\n",
    "\n",
    "# Logisitic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lrClassifier = LogisticRegression()\n",
    "lrScores = cross_validation.cross_val_score(lrClassifier, X, y)\n",
    "print(\"Logistic Regression: %0.2f (+/- %0.2f)\" % (lrScores.mean(), lrScores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per breath: 21.55 seconds\n",
      "Respiratory rate: 0.0278422273782\n"
     ]
    }
   ],
   "source": [
    "breathing_data = pd.read_csv('breathing.csv')\n",
    "\n",
    "BREATHING_FIELD = 'user_acc_z'\n",
    "\n",
    "plt.plot(np.abs(fft.fft(breathing_data[BREATHING_FIELD])))\n",
    "plt.ylim([0,10])\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "wavelength = np.argmax(np.abs(fft.fft(breathing_data[BREATHING_FIELD]))[1000:29000])\n",
    "\n",
    "print 'Time per breath:', float(wavelength)/100, 'seconds'\n",
    "print 'Respiratory rate:', 60.0/wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
